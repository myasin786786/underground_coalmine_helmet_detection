{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK8pc0pLIaIg"
      },
      "source": [
        "<h3>Setting up Environment</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQcfVvyQIaIg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "cd /kaggle/input/mask-rcnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-output": true,
        "id": "sr3JXIcGIaIh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip3 install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16lkC5BnIaIh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "cp -r /kaggle/input/mask-rcnn/Mask_RCNN /kaggle/working/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNu8rGuYIaIi",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "cd /kaggle/working/Mask_RCNN/Mask_RCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Giv0DDtIaIi",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-output": true,
        "id": "t0cFI5h5IaIi",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!python setup.py install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAMNtCdzIaIj"
      },
      "source": [
        "<h3><center>1. Importing Libraries</center></h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ws7KgH26IaIj",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "from mrcnn.config import Config\n",
        "from mrcnn.model import MaskRCNN\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzWffo7_IaIj"
      },
      "source": [
        "<h3><center>2. Configure Model</center></h3>\n",
        "\n",
        "<div style=\"font-family:verdana; word-spacing:1.7px;\">\n",
        "    First, the model must be defined via an instance of the MaskRCNN class. This class requires a configuration object as a parameter. The configuration object defines how the model might be used during training or inference. In this case, the configuration will only specify the number of images per batch, which will be one, and the number of classes to predict. You can see the full extent of the configuration object and the properties that you can override in the config.py file.</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUjqUOFgIaIj",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# define the test configuration\n",
        "class TestConfig(Config):\n",
        "    NAME = \"test\"\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "    NUM_CLASSES = 1 + 80\n",
        "\n",
        "rcnn = MaskRCNN(mode='inference', model_dir='/kaggle/working/Mask_RCNN/Mask_RCNN/',\n",
        "                config=TestConfig())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of_L-jy2IaIk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# draw an image with detected objects\n",
        "def draw_image_with_boxes(filename, boxes_list):\n",
        "    # load the image\n",
        "    data = plt.imread(filename)\n",
        "    # plot the image\n",
        "    plt.imshow(data)\n",
        "    # get the context for drawing boxes\n",
        "    ax = plt.gca()\n",
        "    # plot each box\n",
        "    for box in boxes_list:\n",
        "         # get coordinates\n",
        "        y1, x1, y2, x2 = box\n",
        "         # calculate width and height of the box\n",
        "        width, height = x2 - x1, y2 - y1\n",
        "         # create the shape\n",
        "        rect = Rectangle((x1, y1), width, height, fill=False, color='red')\n",
        "        # draw the box\n",
        "        ax.add_patch(rect)\n",
        "    # show the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTgDH98hIaIk"
      },
      "source": [
        "<h3><center>3. Load Weights & Detect</center></h3>\n",
        "\n",
        "<div style=\"font-family:verdana; word-spacing:1.7px;\">\n",
        "The next step is to load the weights. Now we can make a prediction for our image. <br><br>  We can then make a prediction with the model. Instead of calling predict() as we would on a normal Keras model, will call the detect() function and pass it the single image.<br><br>\n",
        "    The result contains a dictionary for each image that we passed into the detect() function.The keys of the dictionary of note are as follows:\n",
        "    <br>\n",
        "    <ul>\n",
        "        <li>‘rois’: The bound boxes or regions-of-interest (ROI) for detected objects.\n",
        "            <li>‘masks’: The masks for the detected objects.\n",
        "                <li>‘class ids’: The class integers for the detected objects.\n",
        "                    <li>‘scores’: The probability or confidence for each predicted class.\n",
        "    </ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfxjbK6kIaIk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "WEIGHTS = '/kaggle/input/mask-rcnn/mask_rcnn_coco.h5'\n",
        "IMG_PATH = '/kaggle/input/mask-rcnn/elephant.jpg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaQ7-QieIaIk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "rcnn.load_weights(WEIGHTS, by_name=True)\n",
        "\n",
        "img = load_img(IMG_PATH)\n",
        "\n",
        "img = img_to_array(img)\n",
        "\n",
        "results = rcnn.detect([img], verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Olswe54kIaIk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "draw_image_with_boxes(IMG_PATH, results[0]['rois'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7YhXKC7IaIl"
      },
      "source": [
        "<h3><center>1. Parse Annotation File</center></h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLRX91vjIaIl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "ANNOT_PATH = '/content/drive/MyDrive/Helmet_Detection.v1i.coco/_trainannotations.coco.json'\n",
        "Helmet_PATH = '/content/drive/MyDrive/Helmet_Detection.v1i.coco/train'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hXReziNIaIl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import xml.dom.minidom\n",
        "\n",
        "dom = xml.dom.minidom.parse(ANNOT_PATH + '00001.xml')\n",
        "pretty_xml_as_string = dom.toprettyxml()\n",
        "\n",
        "print(pretty_xml_as_string)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XneEjAhZIaIl"
      },
      "source": [
        "<div style=\"font-family:verdana; word-spacing:1.7px;\">\n",
        " We can see that the annotation file contains a size element that describes the shape of the photograph, and object elements describe the bounding boxes for the helmet objects in the image.   \n",
        "    </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHmn0EVaIaIl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from xml.etree import ElementTree\n",
        "\n",
        "def extract_boxes(filename):\n",
        "    # load and parse the file\n",
        "    tree = ElementTree.parse(filename)\n",
        "    # get the root of the document\n",
        "    root = tree.getroot()\n",
        "\n",
        "    boxes = list()\n",
        "\n",
        "    for box in root.findall('.//bndbox'):\n",
        "        xmin = int(box.find('xmin').text)\n",
        "        ymin = int(box.find('ymin').text)\n",
        "        xmax = int(box.find('xmax').text)\n",
        "        ymax = int(box.find('ymax').text)\n",
        "\n",
        "        coors = [xmin, ymin, xmax, ymax]\n",
        "        boxes.append(coors)\n",
        "\n",
        "    width = int(root.find('.//size/width').text)\n",
        "    height = int(root.find('.//size/height').text)\n",
        "\n",
        "    return boxes, width, height\n",
        "\n",
        "boxes, w, h = extract_boxes(ANNOT_PATH + '00001.xml')\n",
        "print(boxes, w, h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths to the COCO format dataset\n",
        "train_path = '/content/drive/MyDrive/Helmet_Detection.v1i.coco/train'\n",
        "val_path = '/content/drive/MyDrive/Helmet_Detection.v1i.coco/valid'\n",
        "train_annotation = '/content/drive/MyDrive/Helmet_Detection.v1i.coco/_trainannotations.coco.json'\n",
        "val_annotation = '/content/drive/MyDrive/Helmet_Detection.v1i.coco/_validannotations.coco.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u25mGUkmIaIm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from mrcnn.utils import Dataset\n",
        "\n",
        "class helmetDataset(Dataset):\n",
        "\n",
        "    def extract_boxes(self, filename):\n",
        "        # load and parse the file\n",
        "        tree = ElementTree.parse(filename)\n",
        "        # get the root of the document\n",
        "        root = tree.getroot()\n",
        "\n",
        "        boxes = list()\n",
        "\n",
        "        for box in root.findall('.//bndbox'):\n",
        "            xmin = int(box.find('xmin').text)\n",
        "            ymin = int(box.find('ymin').text)\n",
        "            xmax = int(box.find('xmax').text)\n",
        "            ymax = int(box.find('ymax').text)\n",
        "\n",
        "            coors = [xmin, ymin, xmax, ymax]\n",
        "            boxes.append(coors)\n",
        "\n",
        "        width = int(root.find('.//size/width').text)\n",
        "        height = int(root.find('.//size/height').text)\n",
        "\n",
        "        return boxes, width, height\n",
        "\n",
        "\n",
        "    def load_dataset(self, img_path, annot_path, is_train=True):\n",
        "        self.add_class(\"dataset\", 1, \"kangaroo\")\n",
        "\n",
        "        for filename in os.listdir(img_path):\n",
        "            image_id = filename[:-4]\n",
        "\n",
        "            # skip bad images\n",
        "            if image_id in ['00090']:\n",
        "                continue\n",
        "\n",
        "            if is_train and int(image_id) >= 150:\n",
        "                continue\n",
        "            if not is_train and int(image_id) < 150:\n",
        "                continue\n",
        "\n",
        "            img_p = img_path + filename\n",
        "            ann_p = annot_path + image_id + '.xml'\n",
        "\n",
        "            self.add_image('dataset', image_id=image_id, path=img_p,\n",
        "                           annotation=ann_p)\n",
        "\n",
        "\n",
        "    def load_mask(self, image_id):\n",
        "        info = self.image_info[image_id]\n",
        "        path = info['annotation']\n",
        "\n",
        "        boxes, w, h = self.extract_boxes(path)\n",
        "\n",
        "        masks = np.zeros([h, w, len(boxes)], dtype='uint8')\n",
        "\n",
        "        class_ids = list()\n",
        "        for i in range(len(boxes)):\n",
        "            box = boxes[i]\n",
        "            row_s, row_e = box[1], box[3]\n",
        "            col_s, col_e = box[0], box[2]\n",
        "\n",
        "            masks[row_s:row_e, col_s:col_e, i] = 1\n",
        "            class_ids.append(self.class_names.index('kangaroo'))\n",
        "\n",
        "        return masks, np.asarray(class_ids, dtype='int32')\n",
        "\n",
        "    def image_reference(self, image_id):\n",
        "        info = self.image_info[image_id]\n",
        "        return info['path']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVyB0H8iIaIm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_set = helmetDatasetDataset()\n",
        "\n",
        "train_set.load_dataset(Helmet_PATH, ANNOT_PATH, is_train=True)\n",
        "\n",
        "train_set.prepare()\n",
        "\n",
        "print('Train: %d' % len(train_set.image_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XA8fm9wOIaIm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "test_set = helmetDatasetDataset()\n",
        "\n",
        "test_set.load_dataset(Helmet_PATH, ANNOT_PATH, is_train=False)\n",
        "\n",
        "test_set.prepare()\n",
        "\n",
        "print('Test: %d' % len(test_set.image_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Nf8EwsSIaIn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "image_id = 0\n",
        "\n",
        "image = train_set.load_image(image_id)\n",
        "print(image.shape)\n",
        "\n",
        "mask, class_ids = train_set.load_mask(image_id)\n",
        "print(mask.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXCjKPknIaIn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "_ = plt.figure(figsize=(15,8))\n",
        "_ = plt.imshow(image)\n",
        "_ = plt.imshow(mask[:,:,0], cmap='gray', alpha=0.5)\n",
        "_ = plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIvg6FmVIaIn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,15))\n",
        "for i in range(9):\n",
        "    plt.subplot(330 + 1 + i)\n",
        "    plt.axis('off')\n",
        "\n",
        "    image = train_set.load_image(i)\n",
        "\n",
        "    mask, _ = train_set.load_mask(i)\n",
        "\n",
        "    plt.imshow(image)\n",
        "    for j in range(mask.shape[2]):\n",
        "        plt.imshow(mask[:,:,j], cmap='gray', alpha=0.3)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee5uFlQPIaIn"
      },
      "source": [
        "<h4>Debugging</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-output": true,
        "id": "AESW_wwkIaIq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "for image_id in train_set.image_ids:\n",
        "\n",
        "    info = train_set.image_info[image_id]\n",
        "\n",
        "    print(info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O33yi3OhIaIq"
      },
      "source": [
        "<h3><center>4. Extract Boundary Boxes</center></h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATI6z_psIaIq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from mrcnn.visualize import display_instances\n",
        "from mrcnn.utils import extract_bboxes\n",
        "\n",
        "image_id = 15\n",
        "\n",
        "image = train_set.load_image(image_id)\n",
        "\n",
        "mask, class_ids = train_set.load_mask(image_id)\n",
        "\n",
        "bbox = extract_bboxes(mask)\n",
        "\n",
        "display_instances(image, bbox, mask, class_ids, train_set.class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZBS4G0OIaIq"
      },
      "source": [
        "<h3><center>5. Configuration for training</center></h3>\n",
        "<div style=\"font-family:verdana; word-spacing:1.7px;\">\n",
        "The pre-defined model architecture and weights can be loaded. This can be achieved by calling the load_weights().<br><br>\n",
        "Class-specific output layers are removed using exclude argument.<br><br>\n",
        "We can also specify what layers to train. In this case, we will only train the heads, that is the output layers of the model.\n",
        "    </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ip5xG_7IaIr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from mrcnn.config import Config\n",
        "\n",
        "class helmetconfig(Config):\n",
        "\n",
        "    NAME = \"helmet_cfg\"\n",
        "    NUM_CLASSES = 1 + 1\n",
        "\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 3\n",
        "\n",
        "    TRAIN_ROIS_PER_IMAGE = 32\n",
        "    MAX_GT_INSTANCES = 7\n",
        "    DETECTION_MAX_INSTANCES = 7\n",
        "\n",
        "    STEPS_PER_EPOCH = 131"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCA6ClmcIaIr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "config = helmetconfig()\n",
        "config.display()\n",
        "\n",
        "model = MaskRCNN(mode='training', model_dir='/kaggle/working/Mask_RCNN/Mask_RCNN/',\n",
        "                 config=config)\n",
        "\n",
        "# load weights (mscoco) and exclude the output layers\n",
        "model.load_weights('/kaggle/input/mask-rcnn/mask_rcnn_coco.h5', by_name=True,\n",
        "                   exclude=[\"mrcnn_class_logits\",\"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBtrzPCoIaIr"
      },
      "source": [
        "<h3><center>5. Training Output layers</center></h3>\n",
        "<div style=\"font-family:verdana; word-spacing:1.7px;\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-output": false,
        "id": "eAh9RT5SIaIr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# train weights (output layers or ✬heads✬)\n",
        "model.train(train_set, test_set, learning_rate=config.LEARNING_RATE,\n",
        "            epochs=1, layers='heads')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T38JYLcAIaIr"
      },
      "source": [
        "<h3><center>6. Evaluate Model</center></h3>\n",
        "\n",
        "<div style=\"font-family:verdana; word-spacing:1.7px;\">\n",
        "The performance of a model for an object recognition task is often evaluated using the mean absolute precision, or mAP. We are predicting bounding boxes so we can determine whether a bounding box prediction is good or not based on how well the predicted and actual bounding boxes overlap. This can be calculated by dividing the area of the overlap by the total area of both bounding boxes, or the intersection divided by the union, referred to as intersection over union, or IoU. A perfect bounding box prediction will have an IoU of 1. It is standard to assume a positive prediction of a bounding box if the IoU is greater than 0.5, e.g. they overlap by 50% or more. Precision refers to the percentage of the correctly predicted bounding boxes (IoU > 0.5) out of all bounding boxes predicted. Recall is the percentage of the correctly predicted bounding boxes (IoU > 0.5) out of all objects in the photo.<br><br>\n",
        "    The average or mean of the average precision (AP) across all of the images in a dataset is called the mean average precision, or mAP. The mask-rcnn library provides a mrcnn.utils.compute_ap to calculate the AP and other metrics for a given images.\n",
        "    </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9FVEwBvIaIr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "WEIGHTS_PATH = '/kaggle/working/Mask_RCNN/Mask_RCNN/helmet_cfg20210224T0711/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-4zQerkIaIr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from mrcnn.utils import compute_ap\n",
        "from mrcnn.model import load_image_gt\n",
        "from mrcnn.model import mold_image\n",
        "\n",
        "class PredictionConfig(Config):\n",
        "    NAME = \"helmet_cfg\"\n",
        "\n",
        "    NUM_CLASSES = 1 + 1\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "def evaluate_model(dataset, model, cfg):\n",
        "    APs = list()\n",
        "    for image_id in dataset.image_ids:\n",
        "        # load image, bounding boxes and masks for the image id\n",
        "        image, _, gt_class_id, gt_bbox, gt_mask = load_image_gt(dataset, cfg,\n",
        "                                                                image_id,use_mini_mask=False)\n",
        "\n",
        "        # convert pixel values (e.g. center)\n",
        "        scaled_image = mold_image(image, cfg)\n",
        "\n",
        "        # convert image into one sample\n",
        "        sample = np.expand_dims(scaled_image, 0)\n",
        "\n",
        "        yhat = model.detect(sample, verbose=0)\n",
        "\n",
        "        # extract results for first sample\n",
        "        r = yhat[0]\n",
        "\n",
        "        AP, _, _, _ = compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
        "                                 r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r[\"masks\"])\n",
        "        APs.append(AP)\n",
        "\n",
        "    mAP = np.mean(APs)\n",
        "    return mAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrLKGq2bIaIr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "cfg = PredictionConfig()\n",
        "\n",
        "model = MaskRCNN(mode='inference', model_dir='/kaggle/working/Mask_RCNN/Mask_RCNN/',\n",
        "                 config=cfg)\n",
        "\n",
        "model.load_weights(WEIGHTS_PATH+'mask_rcnn_helmet_cfg_0000.h5', by_name=True)\n",
        "\n",
        "train_mAP = evaluate_model(train_set, model, cfg)\n",
        "\n",
        "print(\"Train mAP: %.3f\" % train_mAP)\n",
        "\n",
        "test_mAP = evaluate_model(test_set, model, cfg)\n",
        "\n",
        "print(\"Test mAP: %.3f\" % test_mAP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9W2NI8zIaIs",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from mrcnn.model import mold_image\n",
        "\n",
        "def plot_actual_vs_predicted(dataset, model, cfg, n_images=5):\n",
        "    for i in range(n_images):\n",
        "        image = dataset.load_image(i)\n",
        "        mask, _ = dataset.load_mask(i)\n",
        "        scaled_image = mold_image(image, cfg)\n",
        "        sample = np.expand_dims(scaled_image, 0)\n",
        "        yhat = model.detect(sample, verbose=0)[0]\n",
        "\n",
        "        plt.figure(figsize=(20,15))\n",
        "        plt.subplot(n_images, 2, i*2+1)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(image)\n",
        "\n",
        "        if (i==0):\n",
        "            plt.title('Actual')\n",
        "\n",
        "        for j in range(mask.shape[2]):\n",
        "            plt.imshow(mask[:,:,j], cmap='gray', alpha=0.3)\n",
        "\n",
        "        plt.subplot(n_images, 2, i*2+2)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(image)\n",
        "\n",
        "        if (i==0):\n",
        "            plt.title('Predicted')\n",
        "\n",
        "        ax = plt.gca()\n",
        "\n",
        "        for box in yhat['rois']:\n",
        "            y1, x1, y2, x2 = box\n",
        "            width, height = x2 - x1, y2 - y1\n",
        "            rect = Rectangle((x1, y1), width, height, fill=False, color='red')\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYywNybjIaIs",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "plot_actual_vs_predicted(train_set, model, cfg)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
